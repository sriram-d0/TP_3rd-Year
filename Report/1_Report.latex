% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{multirow}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\begin{quote}
\textbf{TERM PAPER REPORT}\\
\textbf{ON}\\
\textbf{Scraping Relevant Images from Web Pages} \textbf{Without
Download}\\
Submitted in partial fulfilment of requirements to

\textbf{CS 363 - Term Paper}\\
BY

\textbf{Batch No. 22}\\
D. Uday Sriram (Y21CS031)\\
D. Girish Sai (Y21CS030)\\
D. Lokesh Kumar (Y21CS028)
\end{quote}

\includegraphics[width=1.8125in,height=1.8125in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image1.png}

JANUARY 2024

\begin{quote}
\textbf{R.V.R. \& J.C. COLLEGE OF ENGINEERING (Autonomous)}
\textbf{(NAAC `A+' Grade)}\\
(Approved by \textbf{AICTE}, Affiliated to Acharya Nagarjuna University)
Chandramoulipuram::Chowdavaram,
\end{quote}

\textbf{GUNTUR -- 522 019}

i

\begin{quote}
\textbf{R.V. R \& J.C. COLLEGE OF ENGINEERING (Autonomous)}
\end{quote}

\textbf{DEPARTMENT OF COMPUTER SCIENCE and ENGINEERING}

\includegraphics[width=1.8125in,height=1.8125in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image1.png}

\textbf{CERTIFICATE}

This is to certify that this Term Paper Report titled ``\textbf{Scraping
Relevant Images from Web Pages Without Download}'' is the study
conducted by \textbf{Dutta Uday Sriram (Y21CS031), Dhulipala Girish Sai
(Y21CS030), Dasari Lokesh Kumar (Y21CS028)} and submitted in partial
fulfilment of the requirements to CS 363 - Term Paper during the
Academic Year 2023-2024.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Mr.S.Karthik}\\
Term Paper Guide
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Dr.Ch.Aparna}\\
Term Paper In-charge
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Dr.M.Sreelatha}\\
Prof. \&Head, CSE
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

ii

\textbf{ACKNOWLEDGEMENT}

The successful completion of any task would be incomplete without a
proper suggestions, guidance and support. Combination of these factors
acts like backbone to our TERM PAPER titled ``\textbf{Scraping Relevant
Images from Web Pages Without Download}''.

We are deeply grateful to our guide, \textbf{Mr.S.Karthik}, for his
unwavering guidance, insightful feedback, and encouragement. His
expertise and dedication have been instrumental in shaping the direction
and quality of this research.

We would like to express our sincere gratitude to \textbf{Dr.Ch.
Aparna}, In-charge for our term paper. Her expertise, guidance, and
support were instrumental in the successful completion of this research.

We express our sincere thanks to \textbf{Dr.M. Sreelatha}, Head of the
Department of Computer Science and Engineering for her encouragement,
support, commitment to enhance research experience.

We are very much thankful to \textbf{Dr. Kolla Srinivas}, Principal of
R.V.R \&J.C College of Engineering, Guntur for providing this supportive
Environment and to engage in research activities.

Finally, we submit our reserves thanks to lab staff in Department of
Computer Science and Engineering for their cooperation, support, for
providing administrative support and technical assistance during
selection.

\begin{quote}
D.Uday Sriram ( Y21CS031 )\\
D.Girish Sai ( Y21CS030 )\\
D.Lokesh Kumar ( Y21CS028 )
\end{quote}

iii

\textbf{ABSTRACT}

Our method makes web image extraction simpler by finding the right
balance between speed and accuracy, all without needing experts to step
in. We group similar web pages together and then guide non-experts in
selecting the relevant images. Rather than relying on extensive image
datasets for training, we use textual data to educate our system.
Through rigorous testing on 200 news websites and a vast corpus of over
600,000 images, our approach consistently outperformed existing
automatic methods, boasting an impressive average f-Measure of 0.958.
Remarkably, this level of performance was achieved with just six
annotated pages per website. Thus, for 200 websites, only 1,200 pages
need to be examined to identify the relevant images. Additionally, by
sidestepping the need for image downloads and leveraging textual data,
our approach not only saves time and storage resources but also
seamlessly integrates with existing web scraping tools. Overall, our
method offers a streamlined solution for web image extraction, combining
efficiency, accuracy, and ease of use for non-experts.

iv

\textbf{CONTENTS}

Page No.

Title Page i

Certificate ii

Acknowledgement iii

Abstract iv

Contents v

List of Tables vi

List of Figures vii

List of abbreviations viii

1Introduction 1

\begin{quote}
1.1Background

1.2Problem statement

1.3Objectives

1.4Limitations of the existing techniques
\end{quote}

2Literature Review 3

3Methodologies Used 16

\begin{quote}
3.1Architecture

3.2Main Topics discussed in the paper
\end{quote}

4Description of Algorithms 19

5Discussion on Results 25

6Conclusion and Future work 29

7References 30

v

\textbf{LIST OF TABLES}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{S.No}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Table No.}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Table Name}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Page No.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
1
\end{quote}
\end{minipage}} & \multirow{2}{*}{5.1} &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Performance Results
\end{quote}
\end{minipage} & 26 \\
& & \multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Comparison of f-Measures
\end{quote}
\end{minipage}} & \multirow{2}{*}{27} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
2
\end{quote}
\end{minipage} & 5.2 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
3
\end{quote}
\end{minipage} & 5.3 & Scaling Analysis of Various Size of URLs & 27 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
4
\end{quote}
\end{minipage} & 5.4 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
The Impacts of Training Dataset Size
\end{quote}
\end{minipage} & 28 \\
\bottomrule()
\end{longtable}

vi

\textbf{LIST OF FIGURES}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{S.No}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Figure No.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\textbf{Figure Name}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Page No.}
\end{minipage} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
1
\end{quote}
\end{minipage} & 3.1.1 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Architecture
\end{quote}
\end{minipage} & 16 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
2
\end{quote}
\end{minipage} & 3.2.1 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Clustering Step
\end{quote}
\end{minipage} & 17 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
3
\end{quote}
\end{minipage} & 3.2.2 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Annotation Step
\end{quote}
\end{minipage} & 18 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
4
\end{quote}
\end{minipage} & 4.1 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Clustering Algorithm
\end{quote}
\end{minipage} & 20 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
5
\end{quote}
\end{minipage} & 4.2 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Annotation Algorithm
\end{quote}
\end{minipage} & 22 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
6
\end{quote}
\end{minipage} & 4.3 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Machine Algorithm
\end{quote}
\end{minipage} & 24 \\
\bottomrule()
\end{longtable}

vii

\textbf{LIST OF ABBRIVATIONS}

1.TC - Text-based Classification

2.DOM - Document object model

3.SVM - Support Vector Machines

4.DBSCAN - Density-based spatial clustering of applications with noise

5.LCS - Longest Common Subsequence

viii

\textbf{1.INTRODUCTION}

\textbf{1.1Background}

Web image extraction poses significant challenges in data retrieval,
especially when considering the vast and dynamic nature of online
content. Traditional methods often rely on manual preparation of
extraction patterns or extensive training datasets, which can be
error-prone and time-consuming. Automatic approaches, while promising,
often require large training datasets and numerous features, making them
difficult to implement without specialized knowledge.

\textbf{1.2Problem Statement}

The challenge lies in effectively extracting relevant images from
diverse web page layouts while differentiating them from irrelevant
images like advertisements and headers. This task is complicated by the
varying sizes and placements of images within different layouts, as well
as the need to navigate the hierarchical structure of HTML elements to
access them. Manual extraction methods rely on expert analysis to
develop extraction patterns, a laborious process prone to errors and
requiring extensive pattern testing across multiple web pages. Automated
extraction approaches offer potential solutions but must contend with
the complexities of accurately identifying relevant images amidst the
multitude of elements on a web page.

\textbf{1.3Objectives}

The proposed research aims to develop a semi-automatic approach for
scraping relevant images from web pages, with a primary focus on
leveraging textual data to enhance performance in terms of accuracy and
computation time. By integrating advanced techniques, this approach
seeks to significantly reduce reliance on manual extraction methods
commonly employed in existing web scraping tools. The goal is to
streamline the extraction process by minimizing the need for expert
intervention, thus making it accessible to a broader range of users.
Central to this approach is the construction of accurate learning models
tailored to website-specific image extraction tasks, achieved with
minimal features and training data.

1

\textbf{1.4Limitations of the Existing Techniques}

\begin{quote}
o\textbf{Expert Dependency in Manual Approaches:}~\\
Manual extraction methods heavily rely on expert users to prepare
extraction patterns, such as regular expressions, CSS selectors, or
XPath expressions. This reliance introduces a bottleneck in the
extraction process, as preparing these patterns is a monotonous and
error-prone task, leading to potential inaccuracies and inefficiencies.

o\textbf{Challenges in Automatic Approaches:}~\\
While automatic extraction approaches aim to eliminate the need for
expert intervention, they still face limitations in effectively scraping
desired data from web pages. Unsupervised methods, relying on tree-based
techniques like tree edit distance matching, are strictly tied to
website structure, limiting their applicability. Supervised approaches,
though more flexible, require well-prepared training datasets tailored
to specific domains, posing challenges in dataset creation and feature
selection.

o\textbf{Limited Focus on Image Prediction:}~\\
Existing studies on data extraction primarily focus on text-based data,
with relatively few addressing the prediction of relevant images.
Unsupervised methods for image prediction often rely on heuristic
measures or simple techniques based on image size, aspect ratio, and
tags, leading to suboptimal performance. While supervised approaches
show promise in improving accuracy, they require extensive parameter
tuning and feature engineering to achieve satisfactory results.

o\textbf{Lack of Integration:}~\\
Existing techniques often operate in isolation, lacking seamless
integration with web scraping tools or broader data extraction
frameworks. This hinders their practical utility and adoption in
real-world applications where streamlined data extraction pipelines are
essential.
\end{quote}

2

\textbf{2.Literature Review}

\begin{quote}
o\textbf{N Aslam, B Tahir, HM Shafiq, MA Mehmood:}~\\
N. Aslam, B. Tahir, H.M. Shafiq, and M.A. Mehmood are prominent
researchers in the field of web data processing and analysis. Their
collective expertise spans various aspects of information retrieval,
data mining, and algorithm development. In their latest research
endeavor, they address the ubiquitous challenge of noise within web
pages, a hindrance to effective data mining and information retrieval
tasks. Building upon existing algorithms such as Boilerpipe and
JustText, the authors introduce a novel approach named Web-AM to tackle
this issue. By extending the capabilities of Boilerpipe and leveraging
the HTML tree structure, Web-AM aims to detect and remove noise from web
articles, enhancing the precision of content extraction processes.
Through rigorous evaluation on benchmark datasets and the creation of
their own corpus, the authors demonstrate the efficacy of Web-AM in
significantly improving precision compared to existing methods.

\textbf{Reference}:

o\textbf{HV Agun, E Uzun:}~\\
Luo HV Agun and E. Uzun introduce a novel approach in their study to
address the challenges associated with automatically extracting relevant
images from web pages. Traditional methods for this task are error-prone
and time-consuming, often requiring extensive manual effort. To overcome
these limitations, the authors propose a fully-automated approach based
on the alignment of regular expressions, marking the first application
of this technique to image extraction. The approach involves a
multi-stage inference process to generate regular expressions from
attribute values of relevant and irrelevant image elements on web pages,
reducing complexity through the application of a constraint on the
Levenshtein distance algorithm

\textbf{Reference}:
\end{quote}

3

\begin{quote}
o\textbf{Ziv Bar-Yossef, Sridhar Rajagopalan:}\\
The formulation and proposition of the template detection problem
signify a critical step towards understanding and addressing the
pervasive use of templates across the web. Through our research, we have
identified templates as fundamental structures that underpin a
significant portion of online content. Our proposed solution, rooted in
the methodology of counting frequent item sets, offers a practical
approach to tackle this challenge.We observe that templates, while
serving as efficient means for structuring information, often pose
challenges to hypertext information retrieval (IR) and data mining (DM)
systems. These challenges stem from the inherent violation of three key
principles governing these systems. Firstly, templates introduce a level
of uniformity that can mislead traditional search algorithms,
undermining their ability to accurately retrieve relevant information.
Secondly, the rigid structure imposed by templates inhibits the
exploration of diverse content, limiting the effectiveness of data
mining techniques in uncovering valuable insights. Lastly, templates can
obscure the natural variability of content, leading to a distortion of
the underlying data distribution and impeding the precision-recall
trade-off.

\textbf{Reference}:

o\textbf{Rodrigo Barbado, Carlos A. Iglesias:}\\
The impact of online reviews on businesses has grown significantly
during last years, being crucial to determine business success in a wide
array of sectors, ranging from restaurants, hotels to e-commerce.
Unfortunately, some users use unethical means to improve their online
reputation by writing fake reviews of their businesses or competitors.
Previous research has addressed fake review detection in a number of
domains, such as product or business reviews in restaurants and hotels.
However, in spite of its economical interest, the domain of consumer
electronics businesses has not yet been thoroughly studied. This article
proposes a feature framework for detecting fake reviews that has been
evaluated in the consumer electronics domain. The contributions are
fourfold: (i) Construction of a dataset for classifying fake reviews in
the consumer electronics domain in four different cities based on
scraping techniques; (ii) definition of a feature framework for fake
review detection; (iii) development of a
\end{quote}

4

\begin{quote}
fake review classification method based on the proposed framework and
(iv) evaluation and analysis of the results for each of the cities under
study. We have reached an 82\% F-Score on the classification task and
the Ada Boost classifier has been proven to be the best one by
statistical means according to the Friedman test.

\textbf{Reference}:

o\textbf{Aanshi Bhardwaj, Veenu Mangat:}\\
World Wide Web (WWW) is now a famous medium by which people all around
the world can spread and gather information of all kind. However, there
is large amount of irrelevant redundant and information on web pages
also. Such information makes various web mining tasks web page crawling,
web page classification, link based ranking and topic distillation
complex. Previously, the relevant content was extracted only from
textual part of web pages. But now-a-days the content on web page is not
only in the text form but also as an image, video or audio. This paper
proposes an improved algorithm for extracting informative content from
web pages i.e. it extracts the relevant content not only as text but
also as images, videos, audios, adobe flash files and online games.
Experiments were conducted on different real websites show that
precision and recall values of our approach is superior to the previous
Word to Leaf Ratio approach.In response to this evolving landscape, this
paper introduces an improved algorithm designed to extract informative
content from web pages in a manner that encompasses diverse media types
beyond text alone. By extending the scope of content extraction to
encompass images, videos, audios, and other multimedia elements, our
approach aims to provide a more holistic representation of the
information available on web pages.

\textbf{Reference}:
\end{quote}

5

\begin{quote}
o\textbf{Lidong Bing, Tak-Lam Wong, and Wai Lam:}\\
We develop an unsupervised learning framework for extracting popular
product attributes from product description pages originated from
different E-commerce Web sites. Unlike existing information extraction
methods that do not consider the popularity of product attributes, our
proposed framework is able to not only detect popular product features
from a collection of customer reviews but also map these popular
features to the related product attributes. One novelty of our framework
is that it can bridge the vocabulary gap between the text in product
description pages and the text in customer reviews. Technically, we
develop a discriminative graphical model based on hidden Conditional
Random Fields. As an unsupervised model, our framework can be easily
applied to a variety of new domains and Web sites without the need of
labeling training samples. Extensive experiments have been conducted to
demonstrate the effectiveness and robustness of our framework.

\textbf{Reference}:

o\textbf{Fadwa Estuka, James Miller:}\\
Database-driven websites and the amount of data stored in their
databases are growing

enormously. Web databases retrieve relevant information in response to
users' queries; the retrieved information is encoded in dynamically
generated web pages as structured data records. Identifying and
extracting retrieved data records is a fundamental task for many
applications, such as competitive intelligence and comparison shopping.
This task is challenging due to the complex underlying structure of such
web pages and the existence of irrelevant information. Numerous
approaches have been introduced to address this problem, but most of
them are HTML-dependent solutions that may no longer be functional with
the continuous development of HTML. Although a few vision-based
techniques have been introduced, various issues exist that inhibit their
performance. To overcome this, we propose a novel visual approach, i.e.,
programming-language-independent, for automatically extracting
structured web data. The proposed approach makes full use of the natural
human tendency of visual object perception and the Gestalt laws of
grouping.
\end{quote}

6

\begin{quote}
\textbf{Reference}:

o\textbf{Nancy Fazal, Khue Nguyen, and Pasi Fränti:}\\
The purpose of this study was to find the efficiency of a web crawler
for finding geotagged photos on the internet. We consider two
alternatives: (1) extracting geolocation directly from the metadata of
the image, and (2) geo-parsing the location from the content of the web
page, which contains an image. We compare the performance of simple
depth-first, breadth-first search, and a selective search using a simple
guiding heuristic. The selective search starts from a given seed web
page and then chooses the next link to visit based on relevance
calculation of all the available links to the web pages they contain in.
Our experiments show that the crawling will find images all over the
world, but the results are rather sparse. Only a fraction of 6845
retrieved images (\textless0.1\%) contained geotag, and among them only
5 percent were able to be attached to geolocation.

\textbf{Reference}:

o\textbf{Leandro Neiva Lopes Figueiredo, Guilherme Tavares de Assis, and
Anderson A. Ferreira:}\\
Extracting data from web pages is an important task for several
applications such as comparison shopping and data mining. Ordinarily,
the data in web pages represent records from a database and are obtained
using a web search. One of the most important steps for extracting
records from a web page is identifying out of the different data
regions, the one containing the records to be extracted. An incorrect
identification of this region may lead to an extraction of incorrect
records. This process is followed by the equally important step of
detecting and correctly splitting the necessary records and their
attributes from the main data region. In this study, we propose a method
for data extraction based on rendering information and an n-gram model
(DERIN) that aims to improve wrapper performance by automatically
selecting the main data region from a search results page and extracting
its records and attributes based on rendering information. The proposed
DERIN method can detect
\end{quote}

7

\begin{quote}
different record structures using techniques based on an n-gram model.
Moreover, DERIN does not require examples to learn how to extract the
data, performs a given domain independently and can detect records that
are not children of the same parent element in the DOM tree.
Experimental results using web pages from several domains show that
DERIN is highly effective and performs well when compared with other
methods.

\textbf{Reference}:

o\textbf{Najlah Gali, Andrei Tabarcea, and Pasi Fränti:}\\
A web page typically contains a blend of information. For a particular
user, only informative data such as main content and representative
images are considered useful, while non-informative data such as
advertisements and navigational banners are not. In this work, we focus
on selecting a representative image that would best represent the
content of a web page. Existing techniques rely on prior knowledge of
website specific templates and on text body. We extract all images,
analyze and rank them according to their features and functionality in
the web page. We select the highest scored image as the representative
image. Our method is fully automated, template independent, and not
limited to a certain type of web pages.

\textbf{Reference}:

o\textbf{Waqar Haider, Yeliz Yesilada:}\\
Table mining on the web is an open problem, and none of the previously
proposed techniques provides a complete solution. Most research focuses
on the structure of the HTML document, but because of the nature and
structure of the web, it is still a challenging problem to detect
relational tables. Web Content Accessibility Guidelines (WCAG) also
cover a wide range of recommendations for making tables accessible, but
our previous work shows that these recommendations are also not
followed; therefore, tables are still inaccessible to disabled people
and automated processing. We propose a new approach to table mining by
not looking at the HTML structure, but rather, the rendered pages by the
browser. The first task in table mining on the web is to classify
relational vs. layout tables, and here, we propose two alternative
\end{quote}

8

\begin{quote}
approaches for that task. We first introduce our dataset, which includes
725 web pages with 9,957 extracted tables. Our first approach extracts
features from a page after being rendered by the browser, then applies
several machine learning algorithms in classifying the layout vs.
relational tables.

\textbf{Reference}:

o\textbf{Wook-Shin Han, Wooseong Kwak, Hwanjo Yu, Jeong-Hoon Lee:}
Extracting tuples from HTML pages has been an important issue in various
web applications. Commercial tuple extraction systems have enjoyed some
success to extract tuples by regarding HTML pages as tree structures and
exploiting XPath queries to find attributes of tuples in the HTML pages.
However, such systems would be vulnerable to small changes on the web
pages. In this paper, we propose a robust tuple extraction system which
utilizes spatial relationships among elements rather than the XPath
queries. Spatial information (e.g., 2-D coordinates) of elements are
maintained in the DOM tree when a web page is rendered in a browser. Our
system regards elements in the rendered page as spatial objects in the
2-D space and executes spatial joins to extract target elements. Since
humans also identify an element in a web page by its relative spatial
location, our system extracting elements by their spatial relationships
could possibly be as robust as manual extraction. To specify and execute
spatial joins, we propose a new query language, RAQuery, based on
topological relationships between any spatial objects in the 2-D space.
We then propose spatial join algorithms that efficiently process the
RAQuery using novel notions of group match and prunable relation group.
We next propose a tuple construction algorithm to build tuples from the
extracted elements obtained by the spatial joins, which can construct
tuples even when there are no boundary HTML elements specified for the
tuples in the web page.

\textbf{Reference}:
\end{quote}

9

\begin{quote}
o\textbf{Jonathan I. Helfman, James D. Hollan:}\\
The web is enormous and constantly growing. User-interfaces for
web-based applications need to make it easy for people to access
relevant information without becoming overwhelmed or disoriented.
Today\textquotesingle s interfaces employ textual representations almost
exclusively, typically organized in lists and hierarchies of web-page
titles or URL taxonomies. Given the ability of images to assist memory
and our frequent exploitation of space in everyday problem solving to
simplify choice, perception, and mental computation, it is surprising
that so little use is made of images and spatial organizations in
accessing and organizing web information. The work we summarize in this
paper suggest that spatial and temporal organization of selectable
images may offer multiple advantages over textual lists of titles and
URLs. We describe several image-based applications, detail basic image
representation techniques, and discuss spatial and temporal strategies
for organization.

\textbf{Reference}:

o\textbf{Chun-Nan Hsu, Ming-Tzung Dung:}\\
Integrating a large number of Web information sources may significantly
increase the utility of the World-Wide Web. A promising solution to the
integration is through the use of a Web Information mediator that
provides seamless, transparent access for the clients. Information
mediators need wrappers to access a Web source as a structured database,
but building wrappers by hand is impractical. Previous work on wrapper
induction is too restrictive to handle a large number of Web pages that
contain tuples with missing attributes, multiple values, variant
attribute permutations, exceptions and typos. This paper presents
SoftMealy, a novel wrapper representation formalism. This representation
is based on a finite-state transducer (FST) and contextual rules. This
approach can wrap a wide range of semistructured Web pages because FSTs
can encode each different attribute permutation as a path

\textbf{Reference}:

1\\
0

o\textbf{Patricia Jiménez, Juan C. Roldán, Rafael Corchuelo:}\\
HTML tables have become pervasive on the Web. Extracting their data
automatically is difficult because finding the relationships between
their cells is not trivial due to the many different layouts, encodings,
and formats available. In this article, we introduce Melva, which is an
unsupervised domain-agnostic proposal to extract data from HTML tables
without requiring any external knowledge bases. It relies on a
clustering approach that helps make label cells apart from value cells
and establish their relationships. We compared Melva to four competitors
on more than 3000 HTML tables from the Wikipedia and the Dresden Web
Table Corpus. The conclusion is that our proposal is 21.70\% better than
the best unsupervised competitor and equals the best supervised
competitor regarding effectiveness, but it is 99.14\% better regarding
efficiency.

\textbf{Reference}:

o\textbf{Yeonjung Kim, Jeahyun Park, Taehwan Kim, Joongmin Choi:} The
main issue for effective Web information extraction is how to recognize
similar patterns in a Web page. Traditionally, it has been shown that
pattern matching by using the HTML DOM tree is more efficient than the
simple string matching approach. Nonetheless, previous tree-based
pattern matching methods have problems by assuming that all HTML tags
have the same values, assigning the same weight to each node in HTML
trees. This paper proposes an enhanced tree matching algorithm that
improves the tree edit distance method by considering the
characteristics of HTML features. We assign different values to
different HTML tree nodes according to their weights for displaying the
corresponding data objects in the browser. Pattern matching of HTML
patterns is done by obtaining the maximum mapping values of two HTML
trees that are constructed with weighted node values from HTML data
objects. Experiments are done over several Web commerce sites to
evaluate the effectiveness of the proposed HTML tree matching algorithm.
\end{quote}

\textbf{Reference}:

\begin{quote}
1\\
1

o\textbf{Christian Kohlschütter, Peter Fankhauser, Wolfgang Nejdl:}\\
In addition to the actual content Web pages consist of navigational
elements, templates, and advertisements. This boilerplate text typically
is not related to the main content, may deteriorate search precision and
thus needs to be detected properly. In this paper, we analyze a small
set of shallow text features for classifying the individual text
elements in a Web page. We compare the approach to complex,
state-of-the-art techniques and show that competitive accuracy can be
achieved, at almost no cost. Moreover, we derive a simple and plausible
stochastic model for describing the boilerplate creation process. With
the help of our model, we also quantify the impact of boilerplate
removal to retrieval performance and show significant improvements over
the baseline. Finally, we extend the principled approach by
straight-forward heuristics, achieving a remarkable detection accuracy.

\textbf{Reference}:

o\textbf{Bing Liu:}\\
Covers all key tasks and techniques of Web search and Web mining, i.e.,
structure mining, content mining, and usage mining.Includes major
algorithms from data mining, machine learning, information retrieval and
text processing, which are crucial for many Web mining tasks.Contains a
rich blend of theory and practice, addressing seminal research ideas and
also looking at the technology from a practical point of view.Second
edition includes new/revised sections on supervised learning, opinion
mining and sentiment analysis, recommender systems and collaborative
filtering, and query log mining.Ideally suited for classes on data
mining, Web mining, Web search, and knowledge discovery in data
bases.Provides internet support with lecture slides and project
problems.
\end{quote}

\textbf{Reference}:

\begin{quote}
1\\
2

o\textbf{Qingtang Liu, Mingbo Shao, Linjing Wu, Gang Zhao, Guilin Fan:}
Main content extraction of web pages is widely used in search engines,
web content aggregation and mobile Internet browsing. However, a mass of
irrelevant information such as advertisement, irrelevant navigation and
trash information is included in web pages. Such irrelevant information
reduces the efficiency of web content processing in content-based
applications. The purpose of this paper is to propose an automatic main
content extraction method of web pages. In this method, we use two
indicators to describe characteristics of web pages: text density and
hyperlink density. According to continuous distribution of similar
content on a page, we use an estimation algorithm to judge if a node is
a content node or a noisy node based on characteristics of the node and
neighboring nodes. This algorithm enables us to filter advertisement
nodes and irrelevant navigation. Experimental results on 10 news
websites revealed that our algorithm could achieve a 96.34\% average
acceptable rate.

\textbf{Reference}:

o\textbf{Pedro Lopez-Garcia, Antonio D. Masegosa, Eneko Osaba:}\\
One of the most challenging issues when facing a classification problem
is to deal with imbalanced datasets. Recently, ensemble classification
techniques have proven to be very successful in addressing this problem.
We present an ensemble classification approach based on feature space
partitioning for imbalanced classification. A hybrid metaheuristic
called GACE is used to optimize the different parameters related to the
feature space partitioning. To assess the performance of the proposal,
an extensive experimentation over imbalanced and real-world datasets
compares different configurations and base classifiers. Its performance
is competitive with that of reference techniques in the literature.

\textbf{Reference}:

1\\
3

o\textbf{Mehul Mahrishi, Sudha Morwal, Nidhi Dahiya, Hanisha Nankani:}
For content based indexing of videos, numerous tools and techniques are
pipe-lined. The major challenge that these techniques face is the
accuracy of index points generated. This paper presents an efficient way
to extract text from video frames along with its timestamps. Text
extraction takes place in a three-step method which combines
pre-processing of extracted Video Frames, similarity measurement for
removing ambiguous frames and finally text extraction using PyTesseract
Optical Character Recognition. The educational videos with presentations
are prioritised. Text extraction is applied upon the headings of that
presentation. These extracted keywords are referred to as Index Points
through out the article.

\textbf{Reference}:

o\textbf{Edimar Manica, Carina Friedrich Dorneles, and Renata Galante.:}
The web is a large repository of entity-pages. An entity-page is a page
that publishes data representing an entity of a particular type, for
example, a page that describes a driver on a website about a car racing
championship. The attribute values published in the entity-pages can be
used for many data-driven companies, such as insurers, retailers, and
search engines. In this article, we define a novel method, called SSUP,
which discovers the entity-pages on the websites. The novelty of our
method is that it combines URL and HTML features in a way that allows
the URL terms to have different weights depending on their capacity to
distinguish entity-pages from other pages, and thus the efficacy of the
entity-page discovery task is increased. SSUP determines the similarity
thresholds on each website without human intervention. We carried out
experiments on a dataset with different real-world websites and a wide
range of entity types. SSUP achieved a 95\% rate of precision and 85\%
recall rate. Our method was compared with two state-of-the-art methods
and outperformed them with a precision gain between 51\% and 66\%.

\textbf{Reference}:

1\\
4

o\textbf{D. C. Reis, P. B. Golgher, A. S. Silva, A. F. Laender:}\\
The Web poses itself as the largest data repository ever available in
the history of humankind. Major efforts have been made in order to
provide efficient access to relevant information within this huge
repository of data. Although several techniques have been developed to
the problem of Web data extraction, their use is still not spread,
mostly because of the need for high human intervention and the low
quality of the extraction results.In this paper, we present a
domain-oriented approach to Web data extraction and discuss its
application to automatically extracting news from Web sites. Our
approach is based on a highly efficient tree structure analysis that
produces very effective results. We have tested our approach with
several important Brazilian on-line news sites and achieved very precise
results, correctly extracting 87.71\% of the news in a set of 4088 pages
distributed among 35 different sites

\textbf{Reference}:

o\textbf{Andrés Soto, Héctor Mora, and Jaime A. Riascos:}\\
Recently, Machine Learning algorithms have been employed to automate
several processes, including software development. However, this action
demands large datasets for training these algorithms. To our knowledge,
there is no tool for generating synthetic datasets that contain HTML
objects (interfaces, codes, wireframe.) Thus, we present the Web
Generator, a software designed to mainly provide web pages, designs, and
content based on the Bootstrap frontend framework. The software delivers
markup code, screenshots, and labels for web elements. We aim to
generate enough material for training and exploring the Machine Learning
approach for automatic web design and development with this software.

\textbf{Reference}:

1\\
5
\end{quote}

\textbf{3.Methodologies Used}

\textbf{3.1Architecture}

\includegraphics[width=6.34722in,height=3.26389in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image2.png}

Fig.3.1.1

\begin{quote}
\textbf{Initialization:}\\
The initialization step of the TC approach involves collecting web pages
from a website. This initial data collection sets the foundation for
subsequent processing, including clustering, annotation, and machine
learning. In the report, it\textquotesingle s important to highlight the
significance of this step as it provides the raw material for building
the training dataset and ultimately training the model.

1\\
6
\end{quote}

\textbf{3.2Main topics discussed in the paper}

\begin{quote}
o\textbf{Clustering step}~\\
Once the web pages are collected, the clustering step aims to identify
groups of web pages that share similar layouts or content structuresnt a
wide range of possibilities. Techniques like Levenshtein distance
calculation and DBSCAN (Density-based spatial clustering of applications
with noise) may be utilized to cluster web pages based on textual data
extracted from HTML elements. The objective is to group web pages with
similar structures together, facilitating the selection of
representative pages for subsequent annotation.
\end{quote}

\includegraphics[width=3.58889in,height=2.46528in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image3.png}

Fig. 3.2.1

\begin{quote}
o\textbf{Annotation Step:}\\
In this step, the algorithm proceeds with the URLs suggested by the
clustering step for further processing Web pages are downloaded from the
selected URLs, and images along with their associated textual data are
extracted. Initially, all images are marked as irrelevant. Users are
then prompted to manually annotate relevant images by inspecting the
content of the web pages. Annotations are updated in the training
dataset, providing labeled data for subsequent model training.

1\\
7

\includegraphics[width=4.08889in,height=3.36806in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image4.png}
\end{quote}

Fig. 3.2.2

\begin{quote}
o\textbf{Machine Learning Step:}\\
The annotated training dataset obtained from the annotation step is
transformed into feature vectors. Each image\textquotesingle s textual
data is represented numerically in the feature vectors. Machine learning
methods, such as classification algorithms or neural networks, are then
applied to construct a learning model. The learning model is trained on
the labeled dataset to predict the relevance of images on web pages,
based on their textual content.

1\\
8
\end{quote}

\textbf{4.Description of Algorithms}

\begin{quote}
\textbf{a.Clustering Algorithm}

Once the web pages are collected, the clustering step aims to identify
groups of web pages that share similar layouts or content structuresnt a
wide range of possibilities. Techniques like Levenshtein distance
calculation and DBSCAN (Density-based spatial clustering of applications
with noise) may be utilized to cluster web pages based on textual data
extracted from HTML elements. The objective is to group web pages with
similar structures together, facilitating the selection of
representative pages for subsequent annotation. The overall procedure of
is provided as follows:

\textbf{Step 1}: Initialize an empty list to store selected URLs for
training data. \textbf{Step 2}: Apply the DBSCAN clustering algorithm to
the input URLs \textbf{Step 3}: Initialize an empty list to store the
length of each cluster.

\textbf{Step 4}: Iterate over each cluster\\
Initialize an empty list to store the length of each cluster.

\textbf{Step 5}: Calculate the remaining training size.

\textbf{Step 6}: If there is remaining training size:\\
Adjust the count of URLs in each cluster.

Select URLs from clusters until the desired training size is reached.
\textbf{Step 7}: Return the selected URLs for training data.
\end{quote}

19

\begin{quote}
\includegraphics[width=5.1875in,height=5.32222in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image5.png}
\end{quote}

Fig. 4.1

20

\begin{quote}
\textbf{b.Annotation Algorithm}

In this step, the algorithm proceeds with the URLs suggested by the
clustering step for further processing Web pages are downloaded from the
selected URLs, and images along with their associated textual data are
extracted. Initially, all images are marked as irrelevant. Users are
then prompted to manually annotate relevant images by inspecting the
content of the web pages. Annotations are updated in the training
dataset, providing labeled data for subsequent model training. The
overall procedure of is provided as follows:

\textbf{Step 1}: Initialize empty TrainingDataset.

\textbf{Step 2}: Loop through each URL in URLs:\\
Download webpage from the URL.

Parse images from the webpage.

\textbf{Step 3}: Loop through each image in Images:\\
Extract textual data from the image\\
Append (textual data, 0) to TrainingDataset.

\textbf{Step 4}: Allow the user to annotate relevant images.

\textbf{Step 5}: Loop through each annotation:\\
Find the corresponding annotation in TrainingDataset. Update the
relevant indicator to 1 for the annotated image. \textbf{Step 6}: Return
the TrainingDataset.
\end{quote}

21

\includegraphics[width=6.42639in,height=5in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image6.png}

Fig. 4.2

22

\begin{quote}
\textbf{c.Machine Learning Algorithm}

The annotated training dataset obtained from the annotation step is
transformed into feature vectors. Each image\textquotesingle s textual
data is represented numerically in the feature vectors. Machine learning
methods, such as classification algorithms or neural networks, are then
applied to construct a learning model. The learning model is trained on
the labeled dataset to predict the relevance of images on web pages,
based on their textual content. The overall procedure of is provided as
follows:

\textbf{Step 1}: Initialize an empty dictionary to store tokens.
\textbf{Step 2}: Initialize feature Vectors with dimensions.
\textbf{Step 3}: Loop through each item in training dataset: Tokenize
the textual data.

For each token in the bag of tokens.

\textbf{Step 4}: Initialize all values in feature vectors to 0.

\textbf{Step 5}: Loop through each item in training dataset: For each
token in the bag of tokens.

\textbf{Step 6}: Return the training dataset.
\end{quote}

23

\includegraphics[width=6.09722in,height=4.6875in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image7.png}

Fig.4.3

24

\textbf{5.Discussion on Results}

\begin{quote}
We encounter the problem known as the imbalanced dataset in the
literature, because the number of irrelevant images is too high in our
dataset. Accuracy is the ratio of both relevant and irrelevant correct
image predictions to all other predictions. For example, if we annotate
all images as irrelevant as the simple prediction model, then the
accuracy value is 0.96. This value may seem very successful at first
glance. However, it has no success in the relevant image prediction. For
this reason, accuracy is generally not a very reliable metric. Instead,
precision, recall, and f-measure are often used in the literature for
performance evaluation. Finally, Log Loss metric is commonly used to
evaluate the performance of learning models that make predictions in the
form of probability values between 0 and 1. Log Loss measures the
discrepancy between the predicted probabilities and the actual class
labels. A learning model with a Log Loss value close to 0 is considered
to have accurately predicted the class labels, while a higher Log Loss
value indicates that the predictions deviate more from the true class
labels.

\textbf{a. Machine Learning Methods}

Several machine learning methods, including Support Vector Machines
(SVM), k-Nearest Neighbors (k-NN), Decision Tree (J48), Random Forests
(RF), and AdaBoost, have been compared to discover a more successful
learning model. SVM is a discriminative machine learning method that
supports different kernel functions, including linear, polynomial, and
Gaussian radial basis function (RBF). In this study, the RBF kernel was
used because it is useful when the data points are not linearly
separable. Besides, this kernel has been implemented and proposed by
Vyas and Frasincar {[}45{]} for this task. k-NN is a non-parametric
method in which weights are calculated for classes for every feature.
This non-parametric method is based on the distance for classification.
J48 (C4.5) is a decision tree classifier that is a map of the possible
classes.
\end{quote}

25

\begin{quote}
\includegraphics[width=5.78333in,height=3.93056in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image8.png}
\end{quote}

Table 5.1

\begin{quote}
\textbf{Comparison of Performance Results on Unsupervised and Supervised
Approaches with the TC Approach}

In this section, we compare the average performance results of various
methods that can be used in both unsupervised and supervised approaches,
as well as our proposed novel approach. These results are the average of
five tests, and the details of these tests will be explained in Sections
5.5 and 5.6. Additionally, we explore five machine learning methodsto
determine the most appropriate method for both supervised and TC
approaches. Table 5 gives the performance results and average
construction/prediction time results. In the unsupervised approaches, we
conducted tests on the entirety of the dataset, which included all 100
web pages from each of the 200 websites. According to Table 5, the
f-Measure of a simple function (width \textgreater{} 300px and height
\textgreater{} 400px) proposed by Bhardwaj and Mangat{[}6{]} is 0.313.
Helfman and Hollan {[}17{]} select a single image, which is a large
image on a web page, for finding a representative image of a web page.
In our experiments, we derive four functions, including the largest
image size (IS), width (W), height (H), and W*H in a web page. The
f-Measure results for the IS and the W of the image functions are 0.395
and 0.396, respectively, which are the best results among the
unsupervised approaches.
\end{quote}

26

\begin{quote}
\includegraphics[width=4.325in,height=3.72222in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image9.png}
\end{quote}

Table 5.2

\begin{quote}
\includegraphics[width=3.70139in,height=3.10694in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image10.png}
\end{quote}

Table 5.3

27

\begin{quote}
\includegraphics[width=5.50556in,height=2.73889in]{vertopal_d97c954ee9c342ac965e01cadf1faa0f/media/image11.png}
\end{quote}

Table 5.4

28

\textbf{6.Conclusion and Future work}

In this study, we introduced a semi-automatic approach for extracting
relevant images from web pages within a single website. We addressed the
challenge of accurately identifying relevant images, which typically
demands significant resources and expertise. By leveraging textual data
from web pages, our approach eliminates the need for manual pattern
generation, offering a more efficient and accessible solution. Our
method demonstrates notable improvements in both performance metrics and
execution time compared to generalized solutions. Furthermore, its
seamless integration into existing web scraping tools underscores its
practical utility for websites with extensive content. Overall, our
study highlights the efficacy of semi-automatic approaches for image
extraction tasks, particularly in contexts where manual efforts are
impractical or time-consuming.

Future Work:\\
Moving forward, our research will focus on several key areas. Firstly,
we aim to extend our approach to address diverse web extraction tasks
beyond image extraction, exploring its adaptability and effectiveness in
various scenarios. Secondly, we plan to refine our methodology by
identifying optimal regular expressions for scraping data from web
pages, incorporating insights gained from analyzing both positive and
negative textual content within HTML elements. Thirdly, we aspire to
generalize our solution by constructing learning models that can be
applied across multiple websites, thereby enhancing its scalability and
versatility. Lastly, we will explore enhancements to the clustering
process, particularly for websites with complex layouts, to further
streamline the data extraction workflow and improve overall performance.
Through these future endeavors, we seek to advance the field of web data
extraction and contribute towards more efficient and robust
methodologies for extracting valuable insights from web content.

29

\textbf{7.References}

{[}1{]} Hayri Volkan Agun and Erdinç Uzun. 2023. An efficient regular
expression inference approach for relevant image

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
extraction.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Appl.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Soft
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Comput.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
135
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(2023),
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
110030.
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

DOI:https://doi.org/10.1016/j.asoc.2023.110030\\
{[}2{]} Julian Alarte, David Insa, Josep Silva, and Salvador Tamarit.
2018. Main content extraction

from heterogeneous webpages. In Web Information Systems Engineering
(WISE'18), Hakim Hacid, Wojciech Cellary, Hua Wang, Hye-Young\\
Paik, and Rui Zhou (Eds.). Springer International Publishing, Cham,
393--407.

{[}3{]} Naseer Aslam, Bilal Tahir, Hafiz Muhammad Shafiq, and Muhammad
Amir Mehmood.

2019. Web-AM: An efficient\\
boilerplate removal algorithm for web articles. In International
Conference on Frontiers of Information Technology\\
(FIT'19). IEEE, 287--2875.
DOI:https://doi.org/10.1109/FIT47737.2019.00061\\
{[}4{]} Ziv Bar-Yossef and Sridhar Rajagopalan. 2002. Template detection
via data mining and its applications. In 11th International Conference
on World Wide Web (WWW'02). Association for Computing Machinery, New
York, NY, 580--591.

DOI:https://doi.org/10.1145/511446.511522\\
{[}5{]} Rodrigo Barbado, Oscar Araque, and Carlos A. Iglesias. 2019. A
framework for fake review detection in online consumer electronics
retailers. Inf. Process. Manag. 56, 4 (2019), 1234--1244.
DOI:https://doi.org/10.1016/j.ipm.2019.03.002\\
{[}6{]} Aanshi Bhardwaj and Veenu Mangat. 2014. An improvised algorithm
for relevant content extraction from web pages.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
J.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Emerg.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Technol.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Web
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Intell.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
6,
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(May
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
2014),
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
226--230.
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

30

DOI:https://doi.org/10.4304/jetwi.6.2.226-230\\
{[}7{]} Lidong Bing, Tak-Lam Wong, and Wai Lam. 2016. Unsupervised
extraction of popular product attributes from ecommerce web sites by
considering customer reviews. ACM Trans.

Internet Technol. 16, 2, Article 12 (Apr. 2016), 17\\
pages. DOI:https://doi.org/10.1145/2857054\\
{[}8{]} Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu.
1996. A density-based algorithm for discovering\\
clustersin large spatial databases with noise. In 2nd International
Conference on Knowledge Discovery and Data Mining\\
(KDD'96). AAAI Press, 226--231.

{[}9{]} Fadwa Estuka and James Miller. 2019. A pure visual approach for
automatically extracting and aligning structured\\
web data. ACM Trans. Internet Technol. 19, 4, Article 51 (Nov. 2019), 26
pages.

DOI:https://doi.org/10.1145/3365376\\
{[}10{]} Nancy Fazal, Khue Nguyen, and Pasi Fränti. 2019. Efficiency of
web crawling for geotagged image retrieval. Webology\\
16 (2019), 16--39. DOI:https://doi.org/10.14704/WEB/V16I1/a177\\
{[}11{]} Emilio Ferrara and Robert Baumgartner. 2011. Automatic wrapper
adaptation by tree edit distance matching. In\\
Combinations of Intelligent Methods and Applications. Springer, UK,
41--54.

{[}12{]} Leandro Neiva Lopes Figueiredo, Guilherme Tavares de Assis, and
Anderson A. Ferreira. 2017. DERIN: A data extraction method based on
rendering information and n-gram. Inf.

Process. Manag. 53, 5 (2017), 1120--1138. DOI:https:\\
//doi.org/10.1016/j.ipm.2017.04.007\\
{[}13{]} Jeffrey E. F. Friedl and Andy Oram. 2002. Mastering Regular
Expressions (2nd ed.).

31

O'Reilly \& Associates, Inc.

{[}14{]} Najlah Gali, Andrei Tabarcea, and Pasi Fränti. 2015. Extracting
representative image from web page. In 11th International Conference on
Web Information Systems and Technologies (WEBIST'15). INSTICC,
SciTePress, Portugal, 411--419.

DOI:https://doi.org/10.5220/0005438704110419\\
{[}15{]} Waqar Haider and Yeliz Yesilada. 2022. Classification of layout
vs. relational tables on the web: Machine learning with\\
rendered pages. ACM Trans. Web 17, 1, Article 1 (Dec. 2022), 23 pages.

DOI:https://doi.org/10.1145/3555349\\
{[}16{]} Wook-Shin Han, Wooseong Kwak, Hwanjo Yu, Jeong-Hoon Lee, and
Min-Soo Kim.

2014. Leveraging spatial join for

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
robust
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
tuple
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
extraction
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
from
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
web
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pages.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Inf.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sci.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
261
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(2014),
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
132--148.
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

DOI:https://doi.org/10.1016/j.ins.2013.09.027\\
{[}17{]} Jonathan I. Helfman and James D. Hollan. 2000. Image
representations for accessing and organizing web information.

In Internet Imaging II, Giordano B. Beretta and Raimondo Schettini
(Eds.), Vol. 4311.

International Society for Optics\\
and Photonics, SPIE, San Jose, CA, 91--101.
DOI:https://doi.org/10.1117/12.411880\\
{[}18{]} Chun-Nan Hsu and Ming-Tzung Dung. 1998. Generating finite-state
transducers for semi-structured data extraction\\
from the web. Inf. Syst. 23, 8 (1998), 521--538.
DOI:https://doi.org/10.1016/S0306-4379(98)00027-1

{[}19{]} Imranul Islam. 2021. Representative Image Extraction from Web
Page. Master's Thesis.

University of Eastern Finland,\\
Faculty of Science and Forestry, Joensuu School of Computing.

32

{[}20{]} Patricia Jiménez, Juan C. Roldán, and Rafael Corchuelo. 2021. A
clustering approach to extract data from HTML

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
tables.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Inf.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Process.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Manag.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
58,
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(2021),
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
102683.
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

DOI:https://doi.org/10.1016/j.ipm.2021.102683\\
{[}21{]} Yeonjung Kim, Jeahyun Park, Taehwan Kim, and Joongmin Choi.
2007. Web information extraction by HTML tree edit\\
distance matching. In International Conference on Convergence
Information Technology (ICCIT'07). IEEE, 2455--2460.

DOI:https://doi.org/10.1109/ICCIT.2007.19\\
{[}22{]} Christian Kohlschütter, Peter Fankhauser, and Wolfgang Nejdl.
2010. Boilerplate detection using shallow text features. In 3rd ACM
International Conference on Web Search and Data Mining (WSDM'10).
Association for Computing\\
Machinery, New York, NY, 441--450.
DOI:https://doi.org/10.1145/1718487.1718542\\
{[}23{]} Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Sov.

Phys. Dokl. 10, 8 (1966), 707--710.

{[}24{]} Bing Liu. 2011. Web Data Mining Exploring Hyperlinks, Contents,
and Usage Data. Springer, Berlin. DOI:https://doi.

org/10.1007/978-3-642-19460-3\\
{[}25{]} Qingtang Liu, Mingbo Shao, Linjing Wu, Gang Zhao, Guilin Fan,
and Jun Li. 2017. Main content extraction from web\\
pages based on node characteristics. J. Comput. Sci. Eng. 11 (06 2017),
39--48. DOI:https://doi.org/10.5626/JCSE.2017.

11.2.39\\
{[}26{]} Pedro Lopez-Garcia, Antonio D. Masegosa, Eneko Osaba, Enrique
Onieva, and Asier 33

Perallos. 2019. Ensemble classification for imbalanced data based on
feature space partitioning and hybrid metaheuristics. Appl. Intell. 49,
8 (Aug.

2019), 2807--2822. DOI:https://doi.org/10.1007/s10489-019-01423-6\\
{[}27{]} Mehul Mahrishi, Sudha Morwal, Nidhi Dahiya, and Hanisha
Nankani. 2021. A framework for index point detection\\
using effective title extraction from video thumbnails. Int. J. Syst.
Assur. Eng. Manag. (June 2021), 1--6. DOI:https:\\
//doi.org/10.1007/s13198-021-01166-z\\
{[}28{]} Edimar Manica, Carina Friedrich Dorneles, and Renata Galante.
2019. Combining URL and HTML features for entity\\
discovery in the web. ACM Trans. Web 13, 4, Article 20 (Dec. 2019), 27
pages.

DOI:https://doi.org/10.1145/3365574\\
{[}29{]} Ion Muslea, Steve Minton, and Craig Knoblock. 1999. A
hierarchical approach to wrapper induction. In 3rd Annual

Conference on Autonomous Agents (AGENTS'99). Association for Computing
Machinery, New York, NY, 190--197.

DOI:https://doi.org/10.1145/301136.301191\\
{[}30{]} D. C. Reis, P. B. Golgher, A. S. Silva, and A. F. Laender.
2004. Automatic web news extraction using tree edit distance.

In 13th International Conference on World Wide Web (WWW'04). Association
for Computing Machinery, New York,\\
NY, 502--511. DOI:https://doi.org/10.1145/988672.988740\\
{[}31{]} Arnaud Sahuguet and Fabien Azavant. 1999. Building light-weight
wrappers for legacy web data-sources using W4F.

In 25th International Conference on Very Large Data Bases (VLDB'99).
Morgan Kaufmann 34

PublishersInc., San Francisco,\\
CA, 738--741.

{[}32{]} Roland Schäfer. 2017. Accurate and efficient general-purpose
boilerplate detection for crawled web corpora. Lang.

Resour. Eval. 51 (2017), 873--889.

{[}33{]} Robert E. Schapire and Yoav Freund. 2012. Boosting: Foundations
and Algorithms. The MIT Press, London, England.

{[}34{]} Andrés Soto, Héctor Mora, and Jaime A. Riascos. 2022. Web
generator: An open-source software for synthetic webbased user interface
dataset generation. SoftwareX 17 (2022), 100985.
DOI:https://doi.org/10.1016/j.softx.2022.100985

35

\end{document}
